{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two classes are copied from the provided lab script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dtree:\n",
    "    \"\"\" A basic Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        fid = open(filename, \"r\")\n",
    "        data = []\n",
    "        d = []\n",
    "        for line in fid.readlines():\n",
    "            d.append(line.strip())\n",
    "        for d1 in d:\n",
    "            data.append(d1.split(\",\"))\n",
    "        fid.close()\n",
    "\n",
    "        self.featureNames = data[0]\n",
    "        self.featureNames = self.featureNames[:-1]\n",
    "        data = data[1:]\n",
    "        self.classes = []\n",
    "        for d in range(len(data)):\n",
    "            self.classes.append(data[d][-1])\n",
    "            data[d] = data[d][:-1]\n",
    "\n",
    "        return data, self.classes, self.featureNames\n",
    "\n",
    "    def classify(self, tree, datapoint):\n",
    "\n",
    "        if type(tree) == type(\"string\"):\n",
    "            # Have reached a leaf\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.featureNames)):\n",
    "                if self.featureNames[i] == a:\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                t = tree[a][datapoint[i]]\n",
    "                return self.classify(t, datapoint)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def classifyAll(self, tree, data):\n",
    "        results = []\n",
    "        for i in range(len(data)):\n",
    "            results.append(self.classify(tree, data[i]))\n",
    "        return results\n",
    "\n",
    "    def make_tree(self, data, classes, featureNames, maxlevel=-1, level=0, forest=0):\n",
    "        \"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "\n",
    "        nData = len(data)\n",
    "        nFeatures = len(data[0])\n",
    "\n",
    "        try:\n",
    "            self.featureNames\n",
    "        except:\n",
    "            self.featureNames = featureNames\n",
    "\n",
    "        # List the possible classes\n",
    "        newClasses = []\n",
    "        for aclass in classes:\n",
    "            if newClasses.count(aclass) == 0:\n",
    "                newClasses.append(aclass)\n",
    "\n",
    "        # Compute the default class (and total entropy)\n",
    "        frequency = np.zeros(len(newClasses))\n",
    "\n",
    "        totalEntropy = 0\n",
    "        totalGini = 0\n",
    "        index = 0\n",
    "        for aclass in newClasses:\n",
    "            frequency[index] = classes.count(aclass)\n",
    "            totalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
    "            totalGini += (float(frequency[index]) / nData) ** 2\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        totalGini = 1 - totalGini\n",
    "        default = classes[np.argmax(frequency)]\n",
    "\n",
    "        if nData == 0 or nFeatures == 0 or (maxlevel >= 0 and level > maxlevel):\n",
    "            # Have reached an empty branch\n",
    "            return default\n",
    "        elif classes.count(classes[0]) == nData:\n",
    "            # Only 1 class remains\n",
    "            return classes[0]\n",
    "        else:\n",
    "\n",
    "            # Choose which feature is best\n",
    "            gain = np.zeros(nFeatures)\n",
    "            ggain = np.zeros(nFeatures)\n",
    "            featureSet = list(range(nFeatures))\n",
    "            if forest != 0:\n",
    "                np.random.shuffle(featureSet)\n",
    "                featureSet = featureSet[0:forest]\n",
    "            for feature in featureSet:\n",
    "                g, gg = self.calc_info_gain(data, classes, feature)\n",
    "                gain[feature] = totalEntropy - g\n",
    "                ggain[feature] = totalGini - gg\n",
    "\n",
    "            bestFeature = np.argmax(gain)\n",
    "            tree = {featureNames[bestFeature]: {}}\n",
    "\n",
    "            # List the values that bestFeature can take\n",
    "            values = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] not in values:\n",
    "                    values.append(datapoint[bestFeature])\n",
    "\n",
    "            for value in values:\n",
    "                # Find the datapoints with each feature value\n",
    "                newData = []\n",
    "                newClasses = []\n",
    "                index = 0\n",
    "                for datapoint in data:\n",
    "                    if datapoint[bestFeature] == value:\n",
    "                        if bestFeature == 0:\n",
    "                            newdatapoint = datapoint[1:]\n",
    "                            newNames = featureNames[1:]\n",
    "                        elif bestFeature == nFeatures:\n",
    "                            newdatapoint = datapoint[:-1]\n",
    "                            newNames = featureNames[:-1]\n",
    "                        else:\n",
    "                            newdatapoint = datapoint[:bestFeature]\n",
    "                            newdatapoint.extend(datapoint[bestFeature + 1:])\n",
    "                            newNames = featureNames[:bestFeature]\n",
    "                            newNames.extend(featureNames[bestFeature + 1:])\n",
    "                        newData.append(newdatapoint)\n",
    "                        newClasses.append(classes[index])\n",
    "                    index += 1\n",
    "\n",
    "                # Now recurse to the next level\n",
    "                subtree = self.make_tree(newData, newClasses, newNames, maxlevel, level + 1, forest)\n",
    "\n",
    "                # And on returning, add the subtree on to the tree\n",
    "                tree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def printTree(self, tree, name):\n",
    "        if type(tree) == dict:\n",
    "            print(name, list(tree.keys())[0])\n",
    "            for item in list(list(tree.values())[0].keys()):\n",
    "                print(name, item)\n",
    "                self.printTree(list(tree.values())[0][item], name + \"\\t\")\n",
    "        else:\n",
    "            print\n",
    "            name, \"\\t->\\t\", tree\n",
    "\n",
    "    def calc_entropy(self, p):\n",
    "        if p != 0:\n",
    "            return -p * np.log2(p)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calc_info_gain(self, data, classes, feature):\n",
    "\n",
    "        # Calculates the information gain based on both entropy and the Gini impurity\n",
    "        gain = 0\n",
    "        ggain = 0\n",
    "        nData = len(data)\n",
    "\n",
    "        # List the values that feature can take\n",
    "\n",
    "        values = []\n",
    "        for datapoint in data:\n",
    "            if datapoint[feature] not in values:\n",
    "                values.append(datapoint[feature])\n",
    "\n",
    "        featureCounts = np.zeros(len(values))\n",
    "        entropy = np.zeros(len(values))\n",
    "        gini = np.zeros(len(values))\n",
    "        valueIndex = 0\n",
    "        # Find where those values appear in data[feature] and the corresponding class\n",
    "        for value in values:\n",
    "            dataIndex = 0\n",
    "            newClasses = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] == value:\n",
    "                    featureCounts[valueIndex] += 1\n",
    "                    newClasses.append(classes[dataIndex])\n",
    "                dataIndex += 1\n",
    "\n",
    "            # Get the values in newClasses\n",
    "            classValues = []\n",
    "            for aclass in newClasses:\n",
    "                if classValues.count(aclass) == 0:\n",
    "                    classValues.append(aclass)\n",
    "\n",
    "            classCounts = np.zeros(len(classValues))\n",
    "            classIndex = 0\n",
    "            for classValue in classValues:\n",
    "                for aclass in newClasses:\n",
    "                    if aclass == classValue:\n",
    "                        classCounts[classIndex] += 1\n",
    "                classIndex += 1\n",
    "\n",
    "            for classIndex in range(len(classValues)):\n",
    "                entropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex]) / np.sum(classCounts))\n",
    "                gini[valueIndex] += (float(classCounts[classIndex]) / np.sum(classCounts)) ** 2\n",
    "\n",
    "            # Computes both the Gini gain and the entropy\n",
    "            gain = gain + float(featureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
    "            ggain = ggain + float(featureCounts[valueIndex]) / nData * gini[valueIndex]\n",
    "            valueIndex += 1\n",
    "        return gain, 1 - ggain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagger:\n",
    "    \"\"\"The bagging algorithm based on the decision tree of Chapter 6\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.tree = dtree()\n",
    "\n",
    "    def bag(self, data, targets, features, nSamples):\n",
    "\n",
    "        nPoints = np.shape(data)[0]\n",
    "        nDim = np.shape(data)[1]\n",
    "        self.nSamples = nSamples\n",
    "\n",
    "        # Compute bootstrap samples\n",
    "        samplePoints = np.random.randint(0, nPoints, (nPoints, nSamples))\n",
    "        classifiers = []\n",
    "\n",
    "        for i in range(nSamples):\n",
    "            sample = []\n",
    "            sampleTarget = []\n",
    "            for j in range(nPoints):\n",
    "                sample.append(data[samplePoints[j, i]])\n",
    "                sampleTarget.append(targets[samplePoints[j, i]])\n",
    "            # Train classifiers\n",
    "            classifiers.append(self.tree.make_tree(sample, sampleTarget, features, 1))\n",
    "\n",
    "        return classifiers\n",
    "\n",
    "    def bagclass(self, classifiers, data):\n",
    "\n",
    "        decision = []\n",
    "        # Majority voting\n",
    "        for j in range(len(data)):\n",
    "            outputs = []\n",
    "            # print data[j]\n",
    "            for i in range(self.nSamples):\n",
    "                out = self.tree.classify(classifiers[i], data[j])\n",
    "                if out is not None:\n",
    "                    outputs.append(out)\n",
    "            # List the possible outputs\n",
    "            out = []\n",
    "            for each in outputs:\n",
    "                if out.count(each) == 0:\n",
    "                    out.append(each)\n",
    "            frequency = np.zeros(len(out))\n",
    "\n",
    "            index = 0\n",
    "            if len(out) > 0:\n",
    "                for each in out:\n",
    "                    frequency[index] = outputs.count(each)\n",
    "                    index += 1\n",
    "                decision.append(out[frequency.argmax()])\n",
    "            else:\n",
    "                decision.append(None)\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Random_Forest main difference in comparison with Bagging is a a splitting method. In Bagging we select all features without any thought, and in Random Forest we select a set of features which are used to create the tree which provides the best classification based on the given feature, after testing and selecting the best one.\n",
    "This selection can be based on gini index, entropy or both.\n",
    "\n",
    "Another important difference, is that the size of a classifier (size of forest) is based on a number of trees, not a number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "class Random_Forest:\n",
    "    \"\"\"The Random Forest algorithm based on the decision tree of Chapter 6\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.tree = dtree()\n",
    "    \n",
    "    \"\"\"Create samples with replacement from datatset\"\"\"\n",
    "    def subsample(self, dataset, classes, ratio):\n",
    "        sample = list()\n",
    "        self.sampleClass = list()\n",
    "        self.nSamples = round(len(dataset) * ratio)\n",
    "        while len(sample) < self.nSamples:\n",
    "            index = randrange(len(dataset))\n",
    "            self.sampleClass.append(classes[index])\n",
    "            sample.append(dataset[index])\n",
    "        return sample\n",
    "\n",
    "    def build_forest(self, data, classes, features, sample_size, forest_size):\n",
    "        \n",
    "        sample = self.subsample( data, classes, sample_size )\n",
    "        \n",
    "        self.Forest_size = forest_size\n",
    "        \n",
    "        classifiers = []\n",
    "        \n",
    "        for i in range(self.Forest_size):\n",
    "            # Train classifiers\n",
    "            classifiers.append(self.tree.make_tree(sample, self.sampleClass, features, 1, 0, 4))\n",
    "\n",
    "        return classifiers\n",
    "\n",
    "    \n",
    "\n",
    "    def forest_prediction(self, classifiers, data):\n",
    "\n",
    "        decision = []\n",
    "        # Majority voting\n",
    "        for j in range(len(data)):\n",
    "            outputs = []\n",
    "            # print data[j]\n",
    "            for i in range(self.Forest_size):\n",
    "                out = self.tree.classify(classifiers[i], data[j])\n",
    "                if out is not None:\n",
    "                    outputs.append(out)\n",
    "            # List the possible outputs\n",
    "            out = []\n",
    "            for each in outputs:\n",
    "                if out.count(each) == 0:\n",
    "                    out.append(each)\n",
    "            frequency = np.zeros(len(out))\n",
    "\n",
    "            index = 0\n",
    "            if len(out) > 0:\n",
    "                for each in out:\n",
    "                    frequency[index] = outputs.count(each)\n",
    "                    index += 1\n",
    "                decision.append(out[frequency.argmax()])\n",
    "            else:\n",
    "                decision.append(None)\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tree = dtree()\n",
    "bag  = bagger()\n",
    "\n",
    "data,classes,features = tree.read_data('car.data')\n",
    "\n",
    "train = data[::2][:]\n",
    "test = data[1::2][:]\n",
    "trainc = classes[::2]\n",
    "testc = classes[1::2]\n",
    "\n",
    "rdforest = Random_Forest()\n",
    "classifiers=rdforest.build_forest(train,trainc,features, 1.0, 50)\n",
    "out = rdforest.forest_prediction(classifiers,test)\n",
    " \n",
    "a = np.zeros(len(out))\n",
    "b = np.zeros(len(out))\n",
    "d = np.zeros(len(out))\n",
    " \n",
    "for i in range(len(out)):\n",
    "    if testc[i] == 'good' or testc[i]== 'v-good':\n",
    "        b[i] = 1\n",
    "        if out[i] == testc[i]:\n",
    "            d[i] = 1\n",
    "    if out[i] == testc[i]:\n",
    "        a[i] = 1\n",
    "print(\"-----\")\n",
    "print(\"Forest\")\n",
    "print(\"Number correctly predicted\",str(np.sum(a)))\n",
    "print(\"Number of testpoints \",str(len(a)))\n",
    "print(\"Percentage Accuracy \",str(np.sum(a)/len(a)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase of accuracy comes with the cost of performance and size of the forest. Bigger forest can yield better results, but takes significantly more time to calculate.  \n",
    "\n",
    "For 10 trees, the accuracy is circa 74%, and for 20 trees the accuracy is circa 78%. The latter result is comparable to what we got in the Bagging method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "Done by Marcin Wadowski, parasited by Artur Kepka."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
